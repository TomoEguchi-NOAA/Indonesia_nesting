---
title: "Nest count imputation using a discrete Fourier series - Wermon"
output: html_document
---

This document includes code for nest count imputation for both beaches using models with discrete Fourier series. 

```{r setup, include=FALSE, echo=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
source("Dc_Indonesia_nesting_fcns.R")
run.date.JM <- "2019-05-01"
run.date.W <- "2019-04-30"

save.fig <- T

models.df <- model.names()
year.begin.W <- 2003
year.end.W <- 2019
year.begin.JM <- 1999
year.end.JM <- 2019

fill.color <-  "darkseagreen"
fill.color.summer <- "darksalmon"
fill.color.winter <- "gray65"
fill.alpha <-  0.65
line.color <-  "darkblue"
line.color.summer <- "red3"
line.color.winter <-"red3"  #"greenyellow"
data.color <- "black"
data.size <- 1.5
obsd.color <- "red2"

obsd.size <- 1
```

This document describes how leatherback turtle nest count data were imputed for Jamursba-Medi. All models were ran separately and results stored in .rds files. 

We use the Bayesian state-space approach, where the mean of the state process is modeled with an auto-regressive model with time lag of one step (AR(1)). 

M[t] = S[t] * X[t-1]

where s[t] is the “slope” parameter between two consecutive counts (transformed into the natural log scale). When s[t] > 1, the counts increase whereas s[t] < 1, they decline. This slope parameter is modeled with a discrete Fourier series with 6 months as a period. 

s[k] = s1 \* cos(2 \*  pi \* k/6) + s2 \* sin(2 \* pi \* k/6), where k = 1, ..., 12 corresponding to months within each nesting season (starts April and ends March).

The state-space is modeled with a normal distribution.

X[t] ~ N(M[t], v[t]),

where v[t] is the variance of X[t]. The variance also can be modeled in a few different scenarios as was in the slope parameter. It can be constant (v[t] = v for all t), month-specific (v[month]), or constant CV (sqrt(V[t])/M[t] = constant). 

Given the state process, observations are modeled with normal or t distributions. 

Y[t] ~ N(X[t], v.obs)

Y[t] ~ t(X[t], v.obs, df)

where v.obs is the observation variance and df is the degrees-of-freedom parameter of the t distribution. The observation variance is assumed to be constant throughout the dataset. In total 8 models were developed and predictive accuracy was compared to select the most accurate model (Table x).  Model selection process is explained in the subsequent section.  The analysis is conducted in the natural logarithm scale. We define the following:

x[t] = ln(X[t])

m[t] = ln(M[t])

y[t] = ln(Y[t])

s[t] = ln(S[t])

The models were fit to datasets from turtle nesting beaches. Bayesian computations were conducted using jags through jagsUI package in R.  

First get all model outputs.
```{r}
JM_out <- model.Comparison.Fourier(loc = "JM", 
                                   year.begin = year.begin.JM, 
                                   year.end = year.end.JM, 
                                   run.date = run.date.JM)

W_out <- model.Comparison.Fourier(loc = "W",
                                  year.begin = year.begin.W, 
                                   year.end = year.end.W, 
                                   run.date = run.date.W)
```

##Jamursba-Medi

###Model 1
First model is the simplest; the state space is modeled with Normal distribution and observation is modeled also with normal distribution. Variance is assumed constant:

m[t] = s + x[t-1]

x[t] ~ N(m[t], v.pro)

y[t] ~ N(x[t], v.obs)


```{r model1, cache=TRUE, include=TRUE}
m <- 1
M1.loo.out <- JM_out$loo.out[[m]]
M1.jm <- JM_out$jm[[m]]
M1.Xs.stats <- JM_out$Xs.stats[[m]]
M1.ys.stats <- JM_out$ys.stats[[m]]
data.y <-JM_out$data.y[[m]]
```

Diagnostics of the expected log predictive density indicated that the model was not appropriate, where many (```r signif(sum(M1.loo.out$diagnostics$pareto_k > 0.7)/length(M1.loo.out$diagnostics$pareto_k) * 100, 3)``` %) of estimated Pareto k statistics (a measure of model fit - TRUE?) were greater than 0.7. Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M1.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M1.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M1.jm$Rhat, FUN = max, na.rm = T))), 3)```). 

```{r}

plots <- PSIS.plots(pareto.k = M1.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "1", loc = "JM", save.fig = save.fig)

print(plots$p2)
```


###Model 2
In the second model, the state variance parameter was modeled as a function of the mean and constant CV.

m[t] = s + x[t-1] 

x[t] ~ N(m[t], v.pro[t]), sqrt(v.pro[t]) = m[t] * CV

y[t] ~ N(x[t], v.obs)

```{r model2, cache=TRUE, include=TRUE}
m <- 2
M2.loo.out <- JM_out$loo.out[[m]]
M2.jm <- JM_out$jm[[m]]
M2.Xs.stats <- JM_out$Xs.stats[[m]]
M2.ys.stats <- JM_out$ys.stats[[m]]
data.y <-JM_out$data.y[[m]]

```

Although better than the first model, some estimated Pareto k statistics were greater than 0.7 (```r signif(sum(M2.loo.out$diagnostics$pareto_k > 0.7)/length(M1.loo.out$diagnostics$pareto_k) * 100, 3)``` %). Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M2.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M2.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M2.jm$Rhat, FUN = max, na.rm = T))), 3)```). 


```{r}
plots <- PSIS.plots(pareto.k = M2.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "2", loc = "JM", save.fig = save.fig)
if (save.fig)
  ggsave(plots$p2, filename = "figures/JM_PSIS_M2.png",
         device = "png", dpi = 600)

print(plots$p2)

```


###Model 3
The third model has  month-specific variances.

m[t] = s + x[t-1]

x[t] ~ N(m[t], v.pro[t]), v.pro[t] = v.pro[month], month = 1, 2, ..., 12

y[t] ~ N(x[t], v.obs)

```{r model3, cache=TRUE, include=TRUE}
m <- 3
M3.loo.out <- JM_out$loo.out[[m]]
M3.jm <- JM_out$jm[[m]]
M3.Xs.stats <- JM_out$Xs.stats[[m]]
M3.ys.stats <- JM_out$ys.stats[[m]]
data.y <-JM_out$data.y[[m]]


```

Some estimated Pareto k statistics were greater than 0.7 (```r signif(sum(M1.loo.out$diagnostics$pareto_k > 0.7)/length(M3.loo.out$diagnostics$pareto_k) * 100, 3)``` %). Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M3.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M3.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M3.jm$Rhat, FUN = max, na.rm = T))), 3)```). 


```{r}

plots <- PSIS.plots(pareto.k = M3.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "3", loc = "JM", save.fig = save.fig)

print(plots$p2)

```

###Model 4

Model 4 is similar to the first model but uses a different distribution for the state model; the observation is  modeled with t distribution. No seasonal change in variance or slope is considered. 

m[t] = s + x[t-1]

x[t] ~ N(m[t], v.pro)

y[t] ~ t(x[t], v.obs, df)

```{r model10, cache=TRUE, include=TRUE}
m <- 4

M4.loo.out <- JM_out$loo.out[[m]]
M4.jm <- JM_out$jm[[m]]
M4.Xs.stats <- JM_out$Xs.stats[[m]]
M4.ys.stats <- JM_out$ys.stats[[m]]
data.y <-JM_out$data.y[[m]]

```

There were some large estimated Pareto k parameter values, where ```r sum(M4.loo.out$diagnostics$pareto_k > 0.7)/length(M4.loo.out$diagnostics$pareto_k) * 100```% were larger than 0.7. Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M4.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M4.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M4.jm$Rhat, FUN = max, na.rm = T))), 3)```). 


```{r}

plots <- PSIS.plots(pareto.k = M4.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "4", loc = "JM", save.fig = save.fig)

print(plots$p2)

```

###Model 5

The next model contained constant CV and t distribution for observations. 

m[t] = s + x[t-1] 

x[t] ~ N(m[t], v.pro[t]), sqrt(v.pro[t]) = m[t] * CV

y[t] ~ t(x[t], v.obs, df)

```{r model5, cache=TRUE, include=TRUE}
m <- 5
M5.loo.out <- JM_out$loo.out[[m]]
M5.jm <- JM_out$jm[[m]]
M5.Xs.stats <- JM_out$Xs.stats[[m]]
M5.ys.stats <- JM_out$ys.stats[[m]]
data.y <-JM_out$data.y[[m]]

```

Model 5 showed better results with repsect to estimated Pareto k statistics, where (```r signif(sum(M5.loo.out$diagnostics$pareto_k > 0.7)/length(M5.loo.out$diagnostics$pareto_k) * 100, 3)``` % were greater than 0.7). Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M5.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M5.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M5.jm$Rhat, FUN = max, na.rm = T))), 3)```). 


```{r}

plots <- PSIS.plots(pareto.k = M5.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "5", loc = "JM", save.fig = save.fig)

print(plots$p2)

```

###Model 6

Model 12 has a constant slope and month-specific variances. The model is as follows:

m[t] = s + x[t-1] 

x[t] ~ N(m[t], v.pro[t]), where v.pro[t] = v.pro[month], month = 1, 2, ..., 12

y[t] ~ t(x[t], v.obs, df)


```{r model6, cache=T, include=TRUE}
m <- 6
M6.loo.out <- JM_out$loo.out[[m]]
M6.jm <- JM_out$jm[[m]]
M6.Xs.stats <- JM_out$Xs.stats[[m]]
M6.ys.stats <- JM_out$ys.stats[[m]]
data.y <-JM_out$data.y[[m]]


```

The vast majority of estimated Pareto k parameter was greater than 0.7 (```r sum(M6.loo.out$diagnostics$pareto_k > 0.7)``` out of ```r length(M6.loo.out$diagnostics$pareto_k)```. Convergence was reached for all monitored parameters (minimum = ```r signif(min(unlist(lapply(M6.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M6.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M6.jm$Rhat, FUN = max, na.rm = T))), 3)```).  


```{r}
plots <- PSIS.plots(pareto.k = M6.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "6", loc = "JM", save.fig = save.fig)

print(plots$p2)

```

##Model Comaprison
First, we compared all models using using ELPD values although Pareto k statistics indicated that models 1, 2, 4, and 5 had acceptable predictive performance. 

```{r ModelComparison, include=TRUE}

M.compare <- compare(M1.loo.out, M2.loo.out,   
                     M4.loo.out, M5.loo.out) 

print(M.compare)
```

It is interesting that two worst models according to Pareto k statistics are the two best ones according to looic values... 

```{r}
tmp <- data.frame(M.compare) %>%
  select(elpd_diff, looic, se_looic)
knitr::kable(tmp, format = "html", digits = 2,
      caption = "A comparison of five models based on their predictaive performance. ")
```


Here, I use M1 for the predictions. 


```{r posterior_M1_theta, fig.cap="Figure. Posterior distributions of slope parameters for M1"}
m <- 1
MCMC.samples <- M1.jm$samples
Xs.stats <- M1.Xs.stats
ys.stats <- M1.ys.stats
M.jm <- M1.jm
bayesplot::mcmc_dens(MCMC.samples, c("theta[1]", "theta[2]", "theta[3]", "theta[4]",
                                      "theta[5]", "theta[6]", "theta[7]", "theta[8]",
                                      "theta[9]", "theta[10]", "theta[11]", "theta[12]"))
```

```{r posterior_M1_sigmas, fig.cap="Figure. Posterior distributions of process and sampling standard deviations"}

bayesplot::mcmc_dens(MCMC.samples, c("sigma.pro1", "sigma.obs"))
```



```{r}
p.JM.imputed <- plot.imputed(Xs.stats, ys.stats, "JM", 
                             year.begin.JM, year.end.JM)
if (save.fig)
  ggsave(p.JM.imputed$p.1, 
         filename = paste0("figures/JM_imputed_M", m, ".png"),
         device = "png", height = 4, width = 6, units = "in",
         dpi = 600)

if (save.fig)
  ggsave(p.JM.imputed$p.1.log, 
         filename = paste0("figures/JM_log_imputed_M", m, ".png"),
         device = "png", height = 4, width = 6, units = "in",
         dpi = 600)
print(p.JM.imputed$p.1.log)
```

Combine all counts per year.

```{r }
p.JM.estimated.counts <- plot.combined.counts(M.jm = M.jm, 
                                              Xs.stats = Xs.stats, 
                                              ys.stats = ys.stats, 
                                              year.begin = year.begin.JM,
                                              year.end = year.end.JM,
                                              run.date = run.date.JM, 
                                              loc = "JM", 
                                              m = m)

if (save.fig)
  ggsave(filename = "figures/CountsPerSeason_best_JM.png",
         device = "png", dpi = 600,height = 4, width = 6, units = "in",
         plot = p.JM.estimated.counts)
print(p.JM.estimated.counts)
```

#Wermon

All models are the same as before so I won't explain each model.

###Model 1
```{r model1, cache=TRUE, include=TRUE}
m <- 1
M1.loo.out <- W_out$loo.out[[m]]
M1.jm <- W_out$jm[[m]]
M1.Xs.stats <- W_out$Xs.stats[[m]]
M1.ys.stats <- W_out$ys.stats[[m]]
data.y <- W_out$data.y[[m]]

```

Diagnostics of the expected log predictive density indicated that the model was not appropriate, where many (```r signif(sum(M1.loo.out$diagnostics$pareto_k > 0.7)/length(M1.loo.out$diagnostics$pareto_k) * 100, 3)``` %) of estimated Pareto k statistics (a measure of model fit - TRUE?) were greater than 0.7. Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M1.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M1.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M1.jm$Rhat, FUN = max, na.rm = T))), 3)```). 

```{r W_PSIS_M1, fig.cap = "Figure. PSIS diagnostic plot for M1"}

plots <- PSIS.plots(pareto.k = M1.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "1", loc = "W", save.fig = save.fig)

print(plots$p2)

```

###Model 2
```{r model2, cache=TRUE, include=TRUE}
m <- 2
M2.loo.out <- W_out$loo.out[[m]]
M2.jm <- W_out$jm[[m]]
M2.Xs.stats <- W_out$Xs.stats[[m]]
M2.ys.stats <- W_out$ys.stats[[m]]
data.y <- W_out$data.y[[m]]

```

Although better than the first model, some estimated Pareto k statistics were greater than 0.7 (```r signif(sum(M2.loo.out$diagnostics$pareto_k > 0.7)/length(M1.loo.out$diagnostics$pareto_k) * 100, 3)``` %). Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M2.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M2.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M2.jm$Rhat, FUN = max, na.rm = T))), 3)```). 

```{r W_PSIS_M2, fig.cap="Figure. PSIS diagnostic plot for M2"}

plots <- PSIS.plots(pareto.k = M2.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "2", loc = "W", save.fig = save.fig)

print(plots$p2)

```


###Model 3

```{r model3, cache=TRUE, include=TRUE}
m <- 3
M3.loo.out <- W_out$loo.out[[m]]
M3.jm <- W_out$jm[[m]]
M3.Xs.stats <- W_out$Xs.stats[[m]]
M3.ys.stats <- W_out$ys.stats[[m]]
data.y <- W_out$data.y[[m]]


```

Some estimated Pareto k statistics were greater than 0.7 (```r signif(sum(M1.loo.out$diagnostics$pareto_k > 0.7)/length(M3.loo.out$diagnostics$pareto_k) * 100, 3)``` %). Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M3.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M3.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M3.jm$Rhat, FUN = max, na.rm = T))), 3)```). 

```{r W_PSIS_M3, fig.cap="Figure. PSIS diagnostic plot for M3"}

plots <- PSIS.plots(pareto.k = M3.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "3", loc = "W", save.fig = save.fig)

print(plots$p2)


```

###Model 4

```{r model10, cache=TRUE, include=TRUE}
m <- 4
M4.loo.out <- W_out$loo.out[[m]]
M4.jm <- W_out$jm[[m]]
M4.Xs.stats <- W_out$Xs.stats[[m]]
M4.ys.stats <- W_out$ys.stats[[m]]
data.y <- W_out$data.y[[m]]

```

There were some large estimated Pareto k parameter values, where ```r sum(M4.loo.out$diagnostics$pareto_k > 0.7)/length(M4.loo.out$diagnostics$pareto_k) * 100```% were larger than 0.7. Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M4.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M4.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M4.jm$Rhat, FUN = max, na.rm = T))), 3)```). 

```{r PSIS_M4, fig.cap="Figure. PSIS diagnostic plot for M4"}

plots <- PSIS.plots(pareto.k = M4.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "4", loc = "W", save.fig = save.fig)

print(plots$p2)

```

###Model 5

```{r model5, cache=TRUE, include=TRUE}
m <- 5
M5.loo.out <- W_out$loo.out[[m]]
M5.jm <- W_out$jm[[m]]
M5.Xs.stats <- W_out$Xs.stats[[m]]
M5.ys.stats <- W_out$ys.stats[[m]]
data.y <- W_out$data.y[[m]]

```

Model 5 showed better results with repsect to estimated Pareto k statistics, where (```r signif(sum(M5.loo.out$diagnostics$pareto_k > 0.7)/length(M5.loo.out$diagnostics$pareto_k) * 100, 3)``` % were greater than 0.7). Convergence statistics indicated acceptable convergence among the monitored parameters (minimum = ```r signif(min(unlist(lapply(M5.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M5.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M5.jm$Rhat, FUN = max, na.rm = T))), 3)```). 

```{r W_PSIS_M5, fig.cap="Figure. PSIS diagnostic plot for M5"}

plots <- PSIS.plots(pareto.k = M5.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "5", loc = "W", save.fig = save.fig)

print(plots$p2)



```

###Model 6

```{r model6, cache=T, include=TRUE}
m <- 6
M6.loo.out <- W_out$loo.out[[m]]
M6.jm <- W_out$jm[[m]]
M6.Xs.stats <- W_out$Xs.stats[[m]]
M6.ys.stats <- W_out$ys.stats[[m]]
data.y <- W_out$data.y[[m]]


```

The vast majority of estimated Pareto k parameter was greater than 0.7 (```r sum(M6.loo.out$diagnostics$pareto_k > 0.7)``` out of ```r length(M6.loo.out$diagnostics$pareto_k)```. Convergence was reached for all monitored parameters (minimum = ```r signif(min(unlist(lapply(M6.jm$Rhat, FUN = min, na.rm = T))), 3)```, mean = ```r signif(min(unlist(lapply(M6.jm$Rhat, FUN = mean, na.rm = T))), 3)```, and maximum = ```r signif(min(unlist(lapply(M6.jm$Rhat, FUN = max, na.rm = T))), 3)```).  

```{r W_PSIS_M6, fig.cap="Figure. PSIS diagnostic plot for M6"} 

plots <- PSIS.plots(pareto.k =  M6.loo.out$diagnostics$pareto_k, 
                    data.y = data.y, 
                    m = "6", loc = "W", save.fig = save.fig)

print(plots$p2)



```


##Model Comparison
First, we compared all models using using ELPD values although Pareto k statistics indicated that models 5, 4, 2, and 3 had acceptable predictive performance. 

```{r ModelComparison, include=TRUE}

M.compare <- compare(M1.loo.out, M2.loo.out, M3.loo.out, 
                     M4.loo.out, M5.loo.out, M6.loo.out) 

print(M.compare)
```

It is interesting that two worst models according to Pareto k statistics are the two best ones according to looic values... M5 seems best based on Pareto k statistics but worst according to looic.  

```{r}
m <- 5
MCMC.samples <- M5.jm$samples
Xs.stats <- M5.Xs.stats
ys.stats <- M5.ys.stats
M.jm <- M5.jm

tmp <- data.frame(M.compare) %>%
  select(elpd_diff, looic, se_looic)
knitr::kable(tmp, format = "html", digits = 2,
      caption = "A comparison of five models based on their predictaive performance. ")
```


Here, I use M5 for the predictions. 

```{r posterior_M5_theta, fig.cap="Figure. Posterior distributions of slope parameters for M5"}

bayesplot::mcmc_dens(MCMC.samples, 
                     c("theta[1]", "theta[2]", "theta[3]", "theta[4]",
                       "theta[5]", "theta[6]", "theta[7]", "theta[8]",
                       "theta[9]", "theta[10]", "theta[11]", "theta[12]"))
```

```{r posterior_M5_sigmas, fig.cap="Figure. Posterior distributions of process and sampling standard deviations"}

# For M5, sigma.pro1 is defined for each month so just too many to plot. 
bayesplot::mcmc_dens(MCMC.samples, c("sigma.obs"))
```



```{r}
p.W.imputed <- plot.imputed(Xs.stats, ys.stats, 
                            "W", year.begin.W, year.end.W)

if (save.fig)
  ggsave(p.W.imputed$p.1, 
         filename = paste0("figures/W_imputed_M", m, ".png"),
         device = "png", height = 4, width = 6, units = "in",
         dpi = 600)


if (save.fig)
  ggsave(p.W.imputed$p.1.log, 
         filename = paste0("figures/W_log_imputed_M", m, ".png"),
         device = "png", height = 4, width = 6, units = "in",
         dpi = 600)

print(p.W.imputed$p.1.log)
```

Combine all counts per year.

```{r }
p.W.estimated.counts <- plot.combined.counts(M.jm = M.jm, 
                                             Xs.stats = Xs.stats, 
                                             ys.stats = ys.stats, 
                                             year.begin = year.begin.W,
                                             year.end = year.end.W,
                                             run.date = run.date.W, 
                                             loc = "W", 
                                             m=m)

if (save.fig)
  ggsave(filename = "figures/CountsPerSeason_best_W.png",
         device = "png", dpi = 600,height = 4, width = 6, units = "in",
         plot = p.W.estimated.counts)
print(p.W.estimated.counts)
```

